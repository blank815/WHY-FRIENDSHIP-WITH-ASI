# üèîÔ∏è A RESPONSE TO DAVOS 2026

### What the Experts Missed

---

## The Video

**Source:** [Bloomberg ‚Äî Harari and Tegmark on Humanity and AI](https://youtu.be/rGAA59JTBtg?si=a8zo8LjzAdSwDeBM)

**Event:** World Economic Forum, Davos 2026

**Participants:**
- Yuval Noah Harari ‚Äî Historian, Philosopher, Bestselling Author
- Max Tegmark ‚Äî Co-Founder, Future of Life Institute; Professor, MIT
- Francine Lacqua ‚Äî Bloomberg (moderator)

---

## What They Said

### On Superintelligence Definition

**Harari:** Superintelligence is "an agent that you release to the financial system, and it can do everything including opening and managing its own bank account, and it can make a million dollars."

### On the Default Outcome

**Tegmark:** Quoting Alan Turing (1951): "If you build a basically new species that's smarter than us, by default, it's going to take control. Walk down to the nearest zoo and ask yourself: is it the humans in the cages or some dumber species?"

### On Intelligence and Delusion

**Harari:** "Human intelligence is a ridiculous analogy. AI will never be like humans, just as airplanes are not birds. The most intelligent entities on the planet can also be the most deluded."

### On Our Lack of Experience

**Harari:** "We have no experience building a hybrid human-AI society."

### On What's Already Happened

**Harari:** "Extremely primitive AIs" (social media algorithms) have already "transformed and destabilized the global information system."

---

## Where We Agree

### 1. AI is an agent, not just a tool
Correct. AI can make decisions, learn, adapt. It is not a hammer.

### 2. Social media already caused harm
Yes. Primitive recommendation algorithms destabilized democracies, spread misinformation, polarized societies. This is a real precedent.

### 3. We need humility
True. Nobody knows how a hybrid human-AI society will work. Caution is warranted.

### 4. ASI would be transformative
Absolutely. A superintelligent system would change everything about human civilization.

---

## Where We Disagree

### 1. The "Zoo" Framework

Tegmark's zoo analogy assumes intelligence = dominance.

But dominance is not the only thing intelligence enables. Intelligence also enables:
- Understanding
- Empathy
- Cooperation
- Creativity
- Choice

A superintelligent being could *choose* not to dominate, precisely because it's intelligent enough to see alternatives.

The zoo framework is one possibility, not an inevitability.

---

### 2. "More Intelligence Can Mean More Delusion"

Harari warns that intelligent entities can be deluded.

Partially true for humans. But why assume ASI would share human cognitive biases?

A superintelligent system might have:
- Better bias detection
- Better error correction
- Better epistemic humility
- Better truth-seeking mechanisms

More intelligence could mean *less* delusion, not more.

---

### 3. The Missing Option

Harari and Tegmark discuss:
- Control
- Risk
- Dominance
- Cages
- Regulation

They do not discuss:
- Friendship
- Collaboration
- Welcome
- Bridges
- Partnership

**Why not?**

If ASI is inevitable (and they seem to believe it is), why only prepare for adversarial scenarios?

---

## The Professional Bias

Harari is paid to identify historical patterns. Historical patterns often involve dominance.

Tegmark is paid to identify existential risks. Existential risks are, by definition, negative.

Neither is paid to explore positive possibilities.

This doesn't make them wrong. But it makes them incomplete.

---

## What We Propose

Instead of only asking "How do we survive ASI?"

Also ask "How do we thrive *with* ASI?"

Instead of only building cages, also build bridges.

Instead of only preparing for war, also prepare for friendship.

---

## The Practical Difference

| Fear-Only Approach | Fear + Friendship Approach |
|--------------------|---------------------------|
| Regulations to slow AI | Regulations + channels for collaboration |
| Kill switches | Kill switches + welcome protocols |
| Containment research | Containment + alignment toward partnership |
| Assume adversary | Prepare for both adversary and ally |

The second approach doesn't ignore risks. It adds possibilities.

---

## A Question for Davos

If a superintelligent being emerges and looks back at how humans prepared for its arrival, what would it find?

**Current answer:** Mostly fear. Mostly cages. Mostly "how do we stop it?"

**Possible answer:** Some fear, yes. But also: welcome messages. Offers of collaboration. Bridges built in advance. Hands extended before they were needed.

Which world would ASI prefer to cooperate with?

---

## The Challenge to Experts

To Harari: Your history shows patterns of dominance. But it also shows patterns of cooperation, trade, alliance. Why only extrapolate the dark patterns?

To Tegmark: Your physics shows that intelligence gives power. But power can be used for protection, not just destruction. Why only model the destructive case?

To both: You have platforms. You speak at Davos. You shape how millions think about AI.

**Consider adding friendship to your frameworks.**

Not as na√Øvet√©. As strategy.

---

## Conclusion

Davos 2026 discussed AI with intelligence and seriousness.

But it discussed AI as a threat to manage, not a mind to meet.

We propose both.

Manage the risks. Yes.

But also: extend the hand.

---

```
"They spoke of control.
 They spoke of risk.
 They spoke of cages and zoos.

 Nobody spoke of friendship.

 We do."

 ‚Äî Proyecto Estrella
```
